---
title: 'Problem Set 12'
author: "Your Name Here"
date: 'Last updated: `r Sys.Date()`'
format:
  html:
    toc: true
    toc_depth: 4
    toc_float: true
    embed-resources: true
---

```{r}
#| label: setup
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(readxl)
library(factoextra)

theme_set(theme_classic())
```


Packages you will need for this problem set:

- `factoextra`

Install them now using the "Packages" pane (if you haven't already).


## Gene expression (RNAseq) on males and females of the Glanville Fritillary Butterfly

Glanville Fritillary butterflies are a model organism for studying dispersal and metapopulation dynamics, because they live in isolated meadows.

![](https://i0.wp.com/www.dorsetbutterflies.com/wordpress/wp-content/uploads/2021/05/Glanville-Fritillary-Mark-Pike-2021-03.jpeg){fig-width=50%}


### Activities

Load in the Butterfly Gene Expression data (`Butterfly_Gene_Expression.csv`). This is a `tibble` of sample id, sex, and 100 normalized gene expression measures (this is trimmed down from an original over 8,000 expression measures). We will perform 100 regressions using `lm()`, predicting each gene expression measure from the sex variable. The `lm()` function can fit multiple Y's at once. See the code below to see how this works for this dataset.

Note this method produces a list of `lm()` results. I have written a function (`getP()` see above) to extract the relevant P-value from a list of this type. We will do this for this set of models and assign the p-values to an object, `obsP`. 

Change `eval: false` to `eval: true` and run the chunk below. You can either change the path to `Butterfly_Gene_Expression.csv` or just load it directly from the web, which is set up for you below.

```{r}
#| eval: true
# FIXME true -> false

getP <- function(fm) {
  sum.set <- summary(fm)
  p.set <- lapply(sum.set, function(x) x[['coefficients']][2, 4])
  return(unlist(p.set))
}

M <- read_csv("https://raw.githubusercontent.com/QM-Life-Sci/QMLS_1/refs/heads/main/data/Butterfly_Gene_Expression.csv",
              show_col_types = FALSE)
Ys <- as.matrix(M[, 3:ncol(M)])
fm <- lm(Ys ~ Sex, data = M)

obsP <- tibble('P' = getP(fm))
```

What is the range of P-values?

```{r}
# FIXME
range(obsP$P)
```


We will visualize the *P*-values by plotting a histogram and a q-q plot. First plot a histogram with 20 `bins`. This will conveniently make bins that have a width of 0.05.

```{r}
# FIXME 

ggplot(obsP, aes(P)) +
  geom_histogram(bins = 20, fill = "navy") +
  labs(x = "P-value", y = "Count")

```

Does it look like there is a signal in the data? I.e., are there more observations in the 0 - 0.05 bin than in the other bins? Or does the distribution look uniform across the range 0 to 1?

> Yes, the first bin is about 2x as large as the next largest. There are more values near 0 than elsewhere.


We will now make a q-q plot, which shows the observed P-values plotted against expected P-values *if the P-values were drawn from a Uniform distribution*.

Make a new tibble `qqP`, with two columns:

1. `observed`: The observed *P*-values converted to -log10(P) and then sorted (`sort()`). This will be the observed values rescaled to make them easier to visualize. 
2. `expected`: Randomly generated values drawn from a uniform distribution using `runif(length(obsP$P), 0, 1)`, also log10 transformed and sorted smallest to largest (`sort()`).

This object now has values for observed and expected P-values.

Finally, plot observed versus expected values and add a 1:1 line (intercept = 0, slope = 1).

```{r}
# FIXME
set.seed(273645)

qqP <- tibble('observed' = sort(-log10(obsP$P)),
              'expected' = sort(-log10(runif(length(obsP$P), 0, 1))))

ggplot(qqP, aes(x = expected, y = observed)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1)

```

If there is no "signal" in the data, then the points (observed vs. predicted) will all fall on the 1:1 line. Based on this visualization, do you think there are *true positives* in this data set?

> Probably. The P-values deviate pretty strongly from the expected line.


Use R's built-in `p.adjust()` function to adjust the P-values using the sequential Bonferroni correction (Holm procedure). You don't have to sort the P-values first, but it can make it easier to pick out ones that remain significant. Print a `tibble` with the observed P-value and sequential Bonferroni corrected P-value. Finally, count the number of P-values that are less than 0.05.

Look at the help for `p.adjust()` for the appropriate syntax.

```{r}
# FIXME
seq_bonf <- p.adjust(obsP$P, method = "holm")
tibble(P = obsP$P,
       seq_bonf) |>
  arrange(P) |>
  filter(seq_bonf <= 0.05)
sum(seq_bonf < 0.05)
```

There were 28 significant P-value in the original dataset (alpha = 0.05). How many remain after this procedure?

> 15 remain

What is the probability that there is one type I error in this set?

> 5%


Repeat the procedure above, but now use the false discovery rate procedure of Benjamini and Hochberg with the default FDR level 5%.

```{r}
# FIXME
fdr <- p.adjust(obsP$P, method = "fdr")
tibble(P = obsP$P,
       fdr) |>
  arrange(P) |>
  filter(fdr <= 0.05)
sum(fdr < 0.05)
```

How many significant tests remain?

> 19

How many of these could be *false positives*?

> 19 * 0.05 = 0.95 = 1


## Sole

The Common Sole (*Solea solea*) is a commercially import (but also potentially threatened due to unsustainable fisheries practices). The file `Sole.xlsx` contains data for the presence/absence of Common Sole in different areas of the Tagus River estuary near Lisbon, Portugal.

![](https://www.nw-ifca.gov.uk/app/uploads/Solea_solea_1-800x600.jpg){fig-align="center"}

These data were collected to help determine the seasonal patterns of sole movement as well as their substrate preferences (i.e., what type of seafloor surface they prefer).

Load the data and look at the structure of the data using `str()` or `glimpse()`.

```{r}
#| echo: true

tf <- tempfile()
download.file("https://raw.githubusercontent.com/QM-Life-Sci/QMLS_1/refs/heads/main/data/Sole.xlsx", destfile = tf)

Sole <- read_excel(tf)
str(Sole)
```

The columns `gravel`, `large_sand`, `med_fine_sand`, and `mud` represent the percentages of these four types of substrate of the sea floor for each sample (they sum to 100).

Use `prcomp()` to carry out a PCA of these four columns. This procedure will calculate a composite score for "substrate".

```{r}
# FIXME

z <- prcomp(~ gravel + large_sand + med_fine_sand + mud,
              data = Sole,
              scale. = TRUE,
              center = TRUE)
```

Use the `summary()` and `print()` methods to determine the percent of variance accounted for by each PC and the relative loadings of each variable.

```{r}
# FIXME

summary(z)
print(z)
```

Use the `fviz_pca_var()` function from the `factoextra` package to make a plot of the PCs.

```{r}
# FIXME

fviz_pca_var(z)
```


What is your interpretation of the PCA?

> PC 1 accounts for 62% of the variation, which is fairly high. Mud loads positively, whereas two kinds of sand load negatively. Gravel doesn't load at all really on PC 1. So PC 1 represents a mud vs. sand axis. PC 2 represents gravel.


A regression model predicting the presence of sole by PC 1 showed a significantly *negative* coefficient. This means that as PC 1 increases, sole are *less* likely to be present.

What type of substrate do sole appear to prefer? Sandy or muddy? Briefly explain your reasoning.

> PC is basically as axis of sand with values in negative PC1 scores to mud in positive PC1 scores. Gravel doesn't load appreciably on PC1. Logistic regression shows that as PC1 score increases, sole are less likely to be present (negative coefficient estimate). So sole are less likely to be present as the substrate gets more muddy and are more likely to be present as the substrate gets more sandy.
