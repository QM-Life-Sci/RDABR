---
title: 'Problem Set 04'
author: "Your Name Here"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r}
#| label: setup
#| message: false
#| warning: false

library(tidyverse)
library(readxl)
library(mvtnorm)
library(ggExtra)

theme_set(theme_classic())
```


# Simulating correlations

In class we looked at a bivariate normal distribution: two variables separately generated from normal distributions: Normal(0, 1). We observed the correlation between these two variables was ~0.

Two variables can be correlated and still maintain bivariate normality (one of the assumptions of a correlation). The code below generates the same distribution that we looked at in class. It is slightly different though:

- We use the `mvtnorm` package function `rmvnorm()`, which generates multivariate normal distributions. Here we make two variables simultaneously.
- We specify the correlation `r` of the two variables (the line: r <- 0), which is included in the matrix `sigma` that specifies the relationships between the simulated values.
- We assign column names to the matrix `x` (because it has none by default).

Run the code and verify that it works. You might have to install the `mvtnorm` and `ggExtra` packages.

```{r}
library(mvtnorm)
library(ggExtra)

set.seed(43237497)

r <- 0
sigma <- matrix(c(1, r, r, 1), ncol = 2)
sigma

x <- rmvnorm(n = 1e4, mean = c(0, 0), sigma = sigma)
colnames(x) <- c("V1", "V2")
head(x)
cor(x[, "V1"], x[, "V2"])

P <- ggplot(as_tibble(x), aes(V1, V2)) +
  geom_point(alpha = 0.25, color = "firebrick") +
  coord_equal()

ggMarginal(P, type = "histogram", fill = "firebrick")
```

You should see that the correlation is close to 0 (~`r round(cor(x[, "V1"], x[, "V2"]), 3)`) and that the plot looks similar to the one from class.


## Activity

Use the code block above to explore the relationship between the correlation and the distributions of the variables.

- Gradually change the correlation `r` to 0.1, 0.2, 0.5, 0.8, 0.9, 0.99. Rerun the code each time. Also try some negative correlations. You only need to change the one line: r <- 0.

What happens to the *joint* distribution of V1 and V2 as the correlation moves away from 0 and gets closer to 1 or -1? 

> It gradually goes from looking like a fuzzy ball of points to a squished fuzzy ball of points. When r is positive it trends from lower left to upper right, when r is negative from upper left to lower right.


In the line `x <- rmvnorm(n = 1e4, mean = c(0, 0), sigma = sigma)`, we set the mean of the normal distribution for V1 and V2 (`mean = c(0, 0)`). Change the values for the means and see how the correlation changes.

How does the pattern change when you change the mean(s)?

> The x and/or y axis labels change, but the overall pattern and the resulting correlations do not change.


What does this result tell you about the correlation between two variables?

> Correlation is independent of the mean. The mean can change but the correlation remains the same.


# Simulating OLS regression

In lecture, we used a set of simulated data to learn how ordinary least squares regression minimizes the sum of the squared residuals. The code we used was

```{r}
set.seed(4)
n <- 30

X <- rnorm(n, mean = 10, sd = 1)
Y <- 2.3 * X + rnorm(n, mean = 1, sd = 1)

M <- tibble(X, Y)

ggplot(M, aes(X, Y)) +
  geom_smooth(formula = y ~ x, method = "lm",
              se = FALSE, color = "darkorange") +
  geom_point(color = "navy", size = 3)

fm <- lm(Y ~ X + 1, data = M)
summary(fm)
```

We have added `geom_smooth()` to add a least squares regression line through the data. Although the slope and intercept of the line is calculated by `ggplot`, it matches the line from `lm()`.

The important part of the summary to look at are these lines:

```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.03541    2.31743  -0.015    0.988    
X            2.39525    0.22035  10.870 1.48e-11 ***
```

The `Estimate` values are the `(Intercept)` (-0.03541) and slope for `X` (2.39525). We will learn about the other values in this table as well as the other parts of the summary in the coming weeks.

We will use this code block to explore how the functional relationship between a predictor and an outcome variables, as well as the variation in each impacts the inferred relationship between them.


## Activity

In the code block above, systematically change the following aspects of the simulation (but only change one thing at a time). For each, describe how the shape of the plot changes as well as the values for intercept and slope. Using factors of 10 will usually allow you to see the patterns.

The slope of the line, by changing 2.3 to larger or smaller values.

> When the slope is small (0.1), there is no relationship, and the values fall far from the line. When the slope is large (20), the points fall very close to the line. Nonetheless, the slope estimate reported in the summary matches pretty well. The intercept doesn't change much.


The range of X, by changing its standard deviation

> When X has a larger range (sd = 10), the points fall right on the line. When X has a small range (sd = 0.1), the points fall far from the line and the slope goes closer to 0. The intercept doesn't change very much.


The random noise parameter (`rnorm(n, mean = 1, sd = 1)`) by altering its mean and standard deviation.

> Adding more noise via a higher mean changes the scale of the y axis, but the slope remains the same (and as expected at 2.3). Adding more noise via a larger standard deviation (sd = 10) moves the points away from the line and changes the slope and intercept. Reducing the noise (sd = 0.1), reduces the spread around the line and results in a slope near 2.3. 

# Going further

A pharmaceutical company is testing a new drug. They have data that shows a large effect size (difference in means) but also substantial variation in the measure in both the treatment group and the control.

If the drug is approved for use, do you expect that all patients given the drug will respond the same (show improvement) and with the same magnitude. Why or why not?

> Because of the large variation in both groups in the experimental testing data, we would expect to see a similar amount of variation in the response to the drug in a real population. Actually we might even expect to see a larger amount of variation when large numbers of people start taking the drug due to person-to-person differences. This pattern is one reason why often severe side-effects and reactions to new drug don't become apparent until after they are approved.


# Lion noses

In class we looked at the correlation between lion age and the proportion of black pigmentation in their nose (or, equally, the proportion of black pigmentation and age). We also used this as motivation for ordinary least squares regression. We will complete that analysis now.

## Activity

The file `LionAges.csv` contains the data. Read the data into an R object. It has two columns:

- `age`: Age of the lion
- `proportion.black`: Proportion of the nose that is black (between 0 and 1).

```{r}
# FIXME

LA <- read_csv("../data/LionAges.csv", show_col_types = FALSE)

LA
```

Make two scatter plots:

1. Proportion black plotted against age
2. Age plotted against proportion black

Feel free to color the points, add axis titles, etc., as you wish.

```{r}
# FIXME

ggplot(LA, aes(x = age, y = proportion.black)) +
  geom_point(size = 4, color = "navy") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(breaks = 1:14)

ggplot(LA, aes(y = age, x = proportion.black)) +
  geom_point(size = 4, color = "navy") +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(breaks = 1:14)
```

Use the `lm()` function to fit a regression where age is predicted by proportion black. Assign the model to a variable. Then generate the `summary()`

```{r}
# FIXME

fm <- lm(age ~ proportion.black,
         data = LA)
summary(fm)
```

What are the slope and the intercept for the best fit line?

> The slope is 10.6 and the intercept is 0.879.


What is you interpretation of the slope of the regression in mathematical terms?

> For a 1 unit increase in proportion black, age increases by 10.6 years.

Why is the slope so large?

> Age is restricted to the range 0 to 1, so the slope of age must be large to accommodate a change from 1-13 years in 1 unit of proportion black.

When we regress age on proportion black, what we are saying is that we know the proportion and want to predict age. Does that make sense in this situation?

> It might be useful to predict age from a photo, so this seems to make sense.

How would the interpretation differ if you regressed proportion black on age?

> In this case, we would be trying to predict the peoportion from the age. That is to say we would already know the age and be trying to determine the proportion. Depending on the scientific question that might make sense. Or maybe not.


# Possum head size

Possums are Australian marsupial (pouched) mammals (not to be confused with North American opossums [also marsupial mammals]). Lindenmayer and colleagues^[Lindenmayer, DB, KL Viggers, RB Cunningham, and CF Donnelly. 1995. Morphological variation among populations of the mountain brushtail possum, *Trichosurus caninus* Ogilby (Phalangeridae, Marsupialia). *Aust. J. Zool.* 43:449.] measured body and head size for 102 animals trapped in the wild in two different areas

Here is a photograph:

![](https://collections.museumsvictoria.com.au/content/media/4/360404-small.jpg)

## Activity

Load the `possum.csv` data set.

```{r}
# FIXME

MBP <- read_csv("../data/possum.csv", show_col_types = FALSE)

glimpse(MBP)
```

What columns does the data contain? See if you can decipher from the column names what they represent. What types are they?

> `site`: A number representing the site the animal was trapped at; `pop`: Character representing the trapping location (Vic or other); `sex`: Character sec of the animal; `age`: Numeric age of the animal; `skull_l`, `skull_w`: Numeric measures of skull length and width; `total_l`: Numeric total length of the possum; `tail_l`: Numeric length of the tail

Make two plots, with points colored by sex in each:

1. Skull length plotted against age
2. Skull width plotted against age

Feel free to add axis labels, etc. Length and width measurements are in millimeters.

```{r}
# FIXME

library(ggsci)

ggplot(MBP, aes(age, skull_l, color = sex)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  scale_color_aaas() +
  labs(x = "Age (y)", y = "Skull Length (mm)")

ggplot(MBP, aes(age, skull_w, color = sex)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  scale_color_aaas() +
  labs(x = "Age (y)", y = "Skull Width (mm)")
```

You will get a couple of warning messages from ggplot:

```
Warning messages:
1: Removed 2 rows containing non-finite outside the scale range
(`stat_smooth()`). 
2: Removed 2 rows containing missing values or values outside the scale range
(`geom_point()`). 
```

See if you can determine what these messages mean.

> There are missing data.

What patterns do you observe in the plot of **skull length vs. age** with respect to age and sex?

> As the animals get older, skull length increases. Males appear to grow faster than females.

What patterns do you observe in the plot of **skull width vs. age** with respect to age and sex?

> As males age, skull width increases. But for females, skull width appears to be roughly constant.


Split the data into two separate tibbles one for females and one for males, by filtering on sex. For an example, see lecture 3-1, where we filtered the NHANES data.

```{r}
# FIXME

MBP_f <- MBP |> filter(sex == "f")
MBP_m <- MBP |> filter(sex == "m")
```

For each sex:

1. Regress skull length on age
2. Regress skull width on age

You will have 4 regressions. You can either assign the regression to an object or simply pipe (`>|`) the `lm()` to `summary()`.

```{r}
# FIXME

lm(skull_l ~ age, data = MBP_f) |> summary()
lm(skull_l ~ age, data = MBP_m) |> summary()

lm(skull_w ~ age, data = MBP_f) |> summary()
lm(skull_w ~ age, data = MBP_m) |> summary()
```

How do the slopes compare between females and males for **skull length**? Does this agree with the plot you made above?

> The slope for females is 0.368 and for males is 0.791. Both slopes are positive, and the slope for males is larger. This is the pattern in the plot.


How do the slopes compare between females and males for **skull width**? Does this agree with the plot you made above?

> The slope for females is 0.082 and for males is 0.773. The slope for females is near zero, which is what it appears to be in the plot. The slope for males is positive and much larger. So again this agrees with the plot.


In the coming weeks, we will learn how to test whether there is a difference in the slopes between males and females.
