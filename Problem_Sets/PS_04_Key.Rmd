---
title: 'Problem Set 04'
author: "Your Name Here"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r}
#| label: setup
#| message: false
#| warning: false

library(tidyverse)
library(readxl)
library(mvtnorm)
library(ggExtra)

theme_set(theme_classic())
```


# Simulating correlations

In class we looked at a bivariate normal distribution: two variables separately generated from normal distributions: Normal(0, 1). We observed the correlation between these two variables was ~0.

Two variables can be correlated and still maintain bivariate normality (one of the assumptions of a correlation). The code below generates the same distribution that we looked at in class. It is slightly different though:

- We use the `mvtnorm` package function `rmvnorm()`, which generates multivariate normal distributions. Here we make two variables simultaneously.
- We specify the correlation `r` of the two variables (the line: r <- 0), which is included to a matrix `sigma` that specifies the relationships between the simulated values.
- We assign column names to the matrix `x` (because it has none by default).

Run the code and verify that it works. You might have to install the `mvtnorm` and `ggExtra` packages.

```{r}
library(mvtnorm)
library(ggExtra)

set.seed(43237497)

r <- 0
sigma <- matrix(c(1, r, r, 1), ncol = 2)
sigma

x <- rmvnorm(n = 1e4, mean = c(0, 0), sigma = sigma)
colnames(x) <- c("V1", "V2")
head(x)
cor(x[, "V1"], x[, "V2"])

P <- ggplot(as_tibble(x), aes(V1, V2)) +
  geom_point(alpha = 0.25, color = "firebrick") +
  coord_equal()

ggMarginal(P, type = "histogram", fill = "firebrick")
```

You should see that the correlation is close to 0 (0.0104) and that the plot looks similar to the one from class.


## Activity

Use the code block above to explore the relationship between the correlation and the distributions of the variables.

- Gradually change the correlation `r` to 0.1, 0.2, 0.5, 0.8, 0.9, 0.99. Rerun the code each time. Also try some negative correlations. You only need to change the one line: r <- 0.

What happens to the *joint* distribution of V1 and V2 as the correlation moves away from 0 and gets closer to 1 or -1? 

> It gradually goes from looking like a ball of points to a squished ball of points. When r is positive it is from lower left to upper right, when r is negative from upper left to lower right.


In the line `x <- rmvnorm(n = 1e4, mean = c(0, 0), sigma = sigma)`, we set the mean of the normal distribution for V1 and V2 (`mean = c(0, 0)`). Change the values for the means and see how the correlation changes.

How does the pattern change when you change the mean(s)?

> The x and/or y axis labels change, but the overall pattern and the resulting correlations do not change.


What does this tell you about the correlation between two variables?

> Correlation is independent of the mean. The mean can change but the correlation remains the same.


# Simulating OLS regression

In lecture, we used a set of simulated data to learn how ordinary least squares regression minimizes the sum of the squared residuals. The code we used was

```{r}
set.seed(4)
n <- 30

X <- rnorm(n, mean = 10, sd = 1)
Y <- 2.3 * X + rnorm(n, mean = 1, sd = 1)

M <- tibble(X, Y)

ggplot(M, aes(X, Y)) +
  geom_smooth(formula = y ~ x, method = "lm",
              se = FALSE, color = "darkorange") +
  geom_point(color = "navy", size = 3)

fm <- lm(Y ~ X + 1, data = M)
summary(fm)
```

We have added `geom_smooth()` to add a least squares regression line through the data. Although the slope and intercept of the line is calculated by `ggplot`, it matches the line from `lm()`.

The important part of the summary to look at are these lines:

```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.03541    2.31743  -0.015    0.988    
X            2.39525    0.22035  10.870 1.48e-11 ***
```

The `Estimate` values are the `(Intercept)` (-0.03541) and slope for `X` (2.39525). We will learn about the other values in this table as well as the other parts of the summary in the coming weeks.

We will use this code block to explore how the functional relationship between a predictor and an outcome variables, as well as the variation in each impacts the inferred relationship between them.


## Activity

In the code block above, systematically change the following aspects of the simulation (but only change one thing at a time). For each, describe how the shape of the plot changes as well as the values for intercept and slope. Using factors of 10 will usually allow you to see the patterns.

The slope of the line, by changing 2.3 to larger or smaller values.

> When the slope is small (0.1), there is no relationship, and the values fall far from the line. When the slope is large (20), the points fall very close to the line. Nonetheless, the slope estimate reported in the summary matches pretty well. The intercept doesn't change much.


The range of X, by changing its standard deviation

> When X has a larger range (sd = 10), the points fall right on the line. When X has a small range (sd = 0.1), the points fall far from the line and the slope goes closer to 0. The intercept doesn't change very much.


The random noise parameter (`rnorm(n, mean = 1, sd = 1)`) by altering its mean and standard deviation.

> Adding more noise via a higher mean changes the scale of the y axis, but the slope remains the same (and as expected at 2.3). Adding more noise via a larger standard deviation (sd = 10) moves the points away from the line and changes the slope and intercept. Reducing the noise (sd = 0.1), reduces the spread around the line and results in a slope near 2.3. 


# Lion noses

In class we looked at the correlation between lion age and the proportion of black pigmentation in their nose (or, equally, the proportion of black pigmentation and age). We also used this as motivation for ordinary least squares regression. We will complete that analysis now.

## Activity

The file `LionAges.csv` contains the data. Read the data into an R object. It has two columns:

- `age`: Age of the lion
- `proportion.black`: Proportion of the nose that is black (between 0 and 1).

```{r}
# FIXME

LA <- read_csv("../data/LionAges.csv", show_col_types = FALSE)

LA
```

Make two scatter plots:

1. Proportion black plotted against age
2. Age plotted against proportion black

Feel free to color the points, add axis titles, etc., as you wish.

```{r}
# FIXME

ggplot(LA, aes(x = age, y = proportion.black)) +
  geom_point(size = 4, color = "navy") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(breaks = 1:14)

ggplot(LA, aes(y = age, x = proportion.black)) +
  geom_point(size = 4, color = "navy") +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(breaks = 1:14)
```

Use the `lm()` function to fit a regression where age is predicted by proportion black. Assign the model to a variable. Then generate the `summary()`

```{r}
# FIXME

fm <- lm(age ~ proportion.black,
         data = LA)
summary(fm)
```

What are the slope and the intercept for the best fit line?

> The slope is 10.6 and the intercept is 0.879.


What is you interpretation of the slope of the regression in mathematical terms?

> For a 1 unit increase in proportion black, age increases by 10.6 years.

Why is the slope so large?

> Age is restricted to the range 0 to 1, so the slope of age must be large to accommodate a change from 1-13 years in 1 unit.

When we regress age on proportion black, what we are saying is that we know the proportion and want to predict age. Does that make sense in this situation?

> It might be useful to predict age from a photo, so this seems to make sense.

How would the interpretation differ if you regressed proportion black on age?

> In this case, we would be trying to predict the peoportion from the age. That is to say we would already know the age and be trying to determine the proportion. Depending on the scientific question that might make sense. Or maybe not.


# Possum head size

Possums are Australian marsupial (pouched) mammals (not to be confused with North American opossums [also marsupial mammals]). Lindenmayer and colleages^[Lindenmayer, D. B., K. L. Viggers, R. B. Cunningham, and C. F. Donnelly. 1995. Morphological variation among populations of the mountain brushtail possum, *Trichosurus caninus* Ogilby (Phalangeridae, Marsupialia). *Aust. J. Zool.* 43:449.] measured body and head size for 102 animals in two different locations.

Here is a photograph:

![](https://collections.museumsvictoria.com.au/content/media/4/360404-small.jpg)

## Activity

```{r}
# FIXME

MBP <- read_csv("../data/possum.csv", show_col_types = FALSE)

glimpse(MBP)
```



```{r}
# FIXME

library(ggsci)

ggplot(MBP, aes(age, skull_l, color = sex)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  scale_color_aaas()

ggplot(MBP, aes(age, skull_w, color = sex)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  scale_color_aaas()
```



See lecture 3-1, where we filtered the NHANES data for an example.

```{r}
# FIXME

MBP_m <- MBP |> filter(sex == "m")
MBP_f <- MBP |> filter(sex == "f")

lm(skull_l ~ age, data = MBP_m) |> summary()
lm(skull_l ~ age, data = MBP_f) |> summary()

lm(skull_w ~ age, data = MBP_m) |> summary()
lm(skull_w ~ age, data = MBP_f) |> summary()
```

