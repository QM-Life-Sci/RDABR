---
title: 'Problem Set 06'
author: "Your Name Here"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
---

```{r}
#| label: setup
#| message: false
#| warning: false

library(tidyverse)

theme_set(theme_classic())
```


## Hypothesis tests

Remember that the definition of a *P*-value is:

> How probable is the observed test statistic (e.g., *t*-statistic) or a test statistic more extreme by chance alone?

When we are considering *t*-statistics (equivalently *t*-values, like what we see in linear model summaries), in practice you calculate *t* for a parameter estimate (e.g., a slope or an intercept) using the equation:

$$t = \frac{\theta - \theta_{null}}{\mbox{SE}_{\theta}}$$

Remember that, almost always, $\theta_{null}$ is 0 (the default for linear model summary tables in R), so the calculation is just the parameter estimate ($\theta$) divided by its standard error. These are the first two columns in the `Coefficients` tables in a `summary()`. The *t*-value with the degrees of freedom is used with `pt()` to calculate the *P*-value.


### Activity

We want to first do a simulation to better understand what the *P*-value in the summary represents. Keep in mind that, for science that we do typically, we do not have the ability to do this kind of simulation where we know the answer ahead of time. 

We can use simulations to explore how statistical models behave under controlled conditions. When simulations give us the answer that we know is there (because we set up the simulation to begin with), it gives us confidence that the same process will work in the real world with real data where we don't know the answer.

Start by generating two sets of n = 20 random numbers drawn from normal distributions. Set the means to 5 in one set and 6 in the other. Both of the standard deviations are 0.9. Call these vectors `x1` and `x2`. There will be 40 total observations between the two sets.

I have set the seed for you, so that our answers will match.

```{r}
set.seed(4578)


```

Calculate the mean of each group and the difference in the means (`x1_mean - x2_mean`). Assign the difference to a variable `d_obs`.

```{r}


```

What are the means and observed difference in the means between `x1` and `x2`?

> 


The code block below carries out a randomization to generate a distribution of differences when x1 and x2 are randomly assigned. Read through the code and the comments, and be sure you understand what each step is doing.

Change `eval: false` to `eval: true`, and run the code. If you have named the variables `x1`, `x2`, and `d_obs`, then it should run without modification.

```{r}
#| eval: false

# Combine x1 and x2 into 1 vector
x <- c(x1, x2)

# Set the number of iterations for the simulation
iters <- 10000

# Create an empty numeric vector to hold the differences
d <- numeric(iters)

# Assign the observed difference to position 1
# This satisfies the "observed test statistic" part of the hypothesis definition.
d[1] <- d_obs

# Run the randomization
for (ii in 2:iters) {                      # Iterate from 2 to iters
  x_shuffled <- sample(x, replace = FALSE) # Shuffle all the elements of x
  x1s <- x_shuffled[1:20]                  # Assign 1-20 to x1s
  x2s <- x_shuffled[21:40]                 # Assign 11-40 to x2s
  d[ii] <- mean(x1s) - mean(x2s)           # Assign the difference of means to the iith position of d
}
```

Now we have a vector of 10,000 differences. The first position holds the observed difference in the groups.

Make a histogram of the differences.

```{r}


```

From just looking at the plot, what is the approximate mean value for the differences? 

> 


To the plot above, add two vertical lines, one at the value of the observed difference and one at the same value but with the opposite sign. You can copy the code to make the histogram from the chunk above.

There are a few examples in the slides for Unit 6 that show you how to add a vertical line. Search for "geom_vline".

```{r}


```

Just from looking at the plot, take a guess about how many times the randomized data returned a difference greater than the observed difference (i.e., more extreme). Remember to consider both tails.

> 


Add up the number of times that the random difference is less than or equal to the observed difference *or* greater than or equal to the opposite sign of the observed difference. I.e., you are summing up the counts in the tails of the distribution.

```{r}


```

Finally calculate the *P*-value for the test of equality of the two means. You can either divide the sum above by `iters` or use `mean()` instead of summing the inequalities.

```{r}


```

What is the *P*-value? This represents the probability that we would observe the difference of -1.22 or anything greater by chance alone.

> 


How likely is it that you would observe a difference like this by chance?

> 


So far we have determined the *P*-value for the difference by simulation. Notice that we did not calculate a *t*-statistic at all -- only the difference in the means between the two groups.

We want to fit a linear model to these data and compare. To do that, we first need to join the observations together into a single tibble along with a grouping variable.

Create a tibble with the combined observations `x1` and `x2` as one variable just called `x`. Also create a grouping variable, e.g., a variable called "Grp" with values "x1" and "x2", which is repeated 20 times each. Look at the help for `rep()` for suggestions. Check that your tibble is correct.

```{r}


```

Finally, fit a linear model where `x` is predicted by the new grouping variable. Print the summary of the linear model.

```{r}


```

This is the same linear model that we used in PS 5.

Compare the hypothesis test for the difference in means between `x1` and `x2` for the linear model (the `Grpx2` coefficient, SE, *t*-value, and *P*-value). Are they similar or different? If they are similar, but not exactly the same, what do you think might explain this discrepancy?

> 


## Stalk-eyed flies

In Problem Set 5 we analyzed a data set where the eye span of stalk-eyed flies was measured in two different diet treatments: corn and cotton. We'll complete this analysis now by looking at hypothesis tests for the linear model.

### Activity

Import the `Stalkies.csv` as in PS 5. Feel free to copy your code or the code from the key.

```{r}


```

#### Exact CIs vs. approximate CIs

Calculate the approximate and exact 95% confidence intervals for the means of the corn and cotton groups. Again, you can copy your code for the approximate CI or use the code from the key.

For the exact confidence interval, you will need to calculate the appropriate *t*-statistic, using the right degrees of freedom for each group separately (consider these as separate confidence intervals). Be sure to use a positive value for *t*, or your CI limits will be reversed.

```{r}


```

How do the exact and approximate CIs compare for this dataset?

> 


For the example we looked at in class, undulation rate in flying snakes, the difference in the two CIs was quite large. How do you explain the close similarity here?

> 


#### Tests of means

Return to the linear model you fit for PS 5. Those instructions were:

1. Fit a linear model using `lm()` where `eye_span` is modeled by `food_source` and assign this to an object.
2. Pass the object that you just created to the function `summary()` to generate the model summary.

Copy the code here and execute it.

```{r}


```

Interpret the *P*-values for the two parameter estimates: `(Intercept)` and `food_sourceCotton`. What hypothesis tests do these values represent? How likely was each parameter estimate to have arisen by chance alone?

> 


#### Using `confint()`

R has a very useful function for calculating confidence intervals for parameter estimates: `confint()`.

Pass the linear model object you saved to the `confint()` function.

```{r}


```

What are the 95% confidence intervals for the "Intercept" and for the difference? Does the intercept agree with the corn CI you calculate above? If it does not, why do you think that might be? Does the difference CI make sense when interpreted in the context of the hypothesis test above?

> 


## Lion noses

In PS 4, we looked at a data set where lion age was related to the proportion of black coloration in the nose. The goal was to be able to predict a lion's age from only a photograph of its nose.

### Activity

Import the `LionAges.csv` data as you did in PS 4 and make a scatterplot of age regressed on proportion of black. Feel free to copy your code over.

```{r}


```

Copy over the linear model you fit where age is modeled by proportion of black.

```{r}


```


What are the slope and the intercept for the OLS regression line?

> 


What is you interpretation of the intercept of the regression in biological terms?

> 


What is you interpretation of the slope of the regression in mathematical terms?

> 


What hypothesis test do the *t*-value and the *P*-value for the `proportion.black` coefficient of 10.6 represent?

> 


Is a slope of 10.6 likely to be observed by chance for these data?

> 


Is proportion of black coloring a good predictor of age in lions?

> 
