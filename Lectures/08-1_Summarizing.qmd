---
title: "Unit 8: Summarizing data and models"
subtitle: "Data Integrity"
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
format:
  clean-revealjs:
    self-contained: true
    chalkboard: false
    standalone: true
    slide-number: true
    show-slide-number: print
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
knitr:
  opts_chunk: 
    echo: true
---

```{r}
#| label: setup
#| echo: false

library(tidyverse)
library(purrr)
library(gt)
library(modelsummary)
library(estimatr)

theme_set(theme_classic(base_size = 20))
```


# Housekeeping {background-color="#40666e"}

- PS 8 due after Spring Break
- PC 2 after Spring break
    - Submit questions for discussion/review before Tuesday 4/1
    - No class meeting on Thursday 4/3


# Plan {background-color="#40666e"}

- Why diagnostics? Checking assumptions
- Errors in data and how to fix them
- Joining data
- Summarizing models


# Analysis paralysis {background-color="#40666e"}

Why are we checking assumptions?

What to do with diagnostics gone wrong?

What can we learn from Picasso?


## Guernica (1937)

![](./images/Picasso_3.png){fig-align="center"}


## The Weeping Woman (1937)

![](./images/Picasso_4.png){fig-align="center"}


## Self-portrait Facing Death (1972)

![](./images/Picasso_5.png){fig-align="center"}


## Untitled (1894)

![](./images/Picasso_1.png){fig-align="center"}


## Untitled (1894)

![](./images/Picasso_2.png){fig-align="center"}


## Learn the most correct way first

- What assumptions matter *most* for an analysis
- *When* assumptions matter for an analysis

:::{.center}
:::{.alert}
Experiment and Practice
:::
:::

![](./images/Picasso_6.png){fig-align="center"}


## Gradations of worries

More important to lesser important:

- Appropriate model
- Small sample size
- Independence
- Normality of residuals
- Constant variance (OLS)
- Equal variance between groups

Don't memorize this.


## The unreasonable robustness of linear models

*Robust*: Not biased despite violations of assumptions

*Biased*: Larger or smaller proportion of significant results than expected when the null hypothesis is true (no difference, slope of 0).

:::{.center}
:::{.alert}
Don't panic when diagnostic tests report "problems". Investigate to understand.
:::
:::


## Convincing ourselves by simulation

- 2 groups; n = 10 in each
- Means and standard deviations are equal
- Calculate difference and *P*-value

```{r}
#| cache: true

set.seed(145784)

iter <- 100000

d <- numeric(length = iter)
P <- numeric(length = iter)

for (ii in 1:iter) {
  x1 <- rnorm(n = 10, mean = 0, sd = 1)
  x2 <- rnorm(n = 10, mean = 0, sd = 1)
  d[ii] <- mean(x1) - mean(x2)
  
  fm <- t.test(x1, x2, var.equal = TRUE)
  P[ii] <- fm$p.value
}
```


## Convincing ourselves by simulation

```{r}
#| echo: false
#| fig-align: center

ggplot(tibble(d), aes(d)) +
  geom_histogram(bins = 30, fill = "navy") +
  labs(x = "Difference in Means", y = "Count")
```


## Convincing ourselves by simulation

What proportion of *P*-values are less than or equal to 0.05?

. . . 
 
```{r}
sum(P <= 0.05) / iter
```

. . .

When there is *no difference* in group means:

- 5% of the time we will find a "significant" *P*-value ($\alpha = 0.05$)
- *All* significant tests are "false-positives"


## Convincing ourselves by simulation

- 2 groups, n = 10 in each
- Means: 0, 0
- Standard deviations:

```{r}
#| echo: false

sds <- cbind(c(1, 1, 1, 1), c(1, 2, 4, 16))
knitr::kable(as_tibble(sds, .name_repair = "unique") |> 
               rename(`Group 1` = `...1`,
                      `Group 2` = `...2`))
```

- *t*-test assuming equal variances
- 100000 iterations: Proportion of *P*-values <= 0.05?

:::{.center}
:::{.alert}
What do you predict?
:::
:::


## Simulation

```{r}
#| cache: true

set.seed(82346)

t_tester <- function(m1, m2, sd1, sd2, ss = 10, niter = 100000) {
  ps <- logical(length = niter)
  for (jj in 1:niter) {
    fm <- t.test(rnorm(ss, m1, sd1),
                 rnorm(ss, m2, sd2),
                 var.equal = TRUE)
    ps[jj] <- fm$p.value < 0.05
  }
  return(mean(ps))
}

D <- tibble(m1 = 0,
            m2 = 0,
            sd1 = c(1, 1, 1, 1),
            sd2 = c(1, 2, 4, 16))
```


## Simulation

```{r}
#| cache: true

pmap_dbl(.l = D,
         .f = t_tester)
```

- Equal standard deviation: ~5%
- 2x difference: ~5.6%
- 4x difference: ~6.1%
- 16x difference: ~6.5%

What does this mean if the observed *P*-value is 0.048? Or if *P* = 0.0000048?


## Using Welch's Correction

```{r}
#| echo: true
#| cache: true

set.seed(234978)
t_tester_welch <- function(m1, m2, sd1, sd2, ss = 10, niter = 100000) {
  ps <- logical(length = niter)
  for (jj in 1:niter) {
    mod <- t.test(rnorm(ss, m1, sd1),
                  rnorm(ss, m2, sd2),
                  var.equal = FALSE)
    ps[jj] <- mod$p.value < 0.05
  }
  return(mean(ps))
}

pmap_dbl(.l = D, # reuse D
         .f = t_tester_welch)
```


# Questions {background-color="#f8c471"}


# Data Integrity {background-color="#40666e"}


## Goals for curating data

- More than just no errors
- [FAIR](https://www.go-fair.org/fair-principles/): Findable, Accessible, Interoperable, Reusable
- Have a traceable path for all changes to data
    - Similar to code giving a step-by-step set of repeatable analyses


## Isolating raw data from analysis

- Separate data cleaning/preparation from analysis
    - Clean once, analyze often
- Read raw data
    - Process / fix
- Write cleaned data to a new file (e.g., `Processed_data` directory)
    - Read into analysis code


## Finding and fixing errors

- Develop a plan for data entry
- Data exploration and initial plotting
- Check your lab notebook and/or data sheet

Two options for errors:

1. Fix in the original data
2. Fix in code and re-export


## Some examples

![](./images/M_co_go.png){fig-align="center"}


## Fix in code and re-export



## Locating errors

```{r}
#| echo: false
#| fig-align: center

STB <- read_csv("https://raw.githubusercontent.com/QM-Life-Sci/RDABR/refs/heads/main/data/Swallowtail.csv", show_col_types = FALSE) |> 
  mutate(ID = 1:n(),
         Forewing_area = replace(Forewing_area, Forewing_area == 5.523, 55.23)) |> 
  relocate(ID)

ggplot(STB, aes(Forewing_area, Hindwing_area)) +
  geom_point(size = 3, color = "darkgreen")
```


```{r}
STB |> arrange(desc(Forewing_area)) |> slice(1:5)
STB$Forewing_area[176]
```


## Replacing values: `replace()`

```{r}
STB_correct <- STB |> 
  mutate(Forewing_area = replace(Forewing_area, Forewing_area == 55.23, 5.523))

ggplot(STB_correct, aes(Forewing_area, Hindwing_area)) +
  geom_point(size = 3, color = "darkgreen")
```


## Joining data

![](./images/Malurus_cyaneus.jpg){fig-align="center"}


- ~30 years of breeding data for ~1900 female Superb Fairywrens (*Malurus cyaneus*)
- Individuals tracked for many years (number of fledged offspring, reproductive senescence, etc.)
- Similar data for males


## Relational Data

- Try not to store redundant data (same data in multiple files) - prone to errors and mismatches
- Compartmentalize data sets
- Join them together via one or more columns containing values that connect one data set to each other

Relations are always built between pairs of tables. 

- What column(s) uniquely defines these relationships?


## Datasets

```{r}
#| echo: false

wren_surv <- read_csv(
  "https://raw.githubusercontent.com/QM-Life-Sci/QMLS_1/refs/heads/main/data/wren_female_surv.csv",
  show_col_types = FALSE,
  col_types = "-cn-nc") |> 
  rename(longevity = long_years) |> 
  arrange(ID, longevity) |> 
  group_by(ID) |> 
  summarise(final_obs_year = max(year),
            longevity = max(longevity),
            sex = sex[1])

wren_repro <- read_csv(
  "https://raw.githubusercontent.com/QM-Life-Sci/QMLS_1/refs/heads/main/data/wren_female_repro.csv",
  show_col_types = FALSE,
  col_types = "-cnnn-n-") |> 
  rename(n_offspring = offspring_inde,
         senescent = final_breed) |> 
  mutate(mass = rnorm(n(), mean = 10.5, sd = 1) |> round(digits = 1))
```

```{r}
#| echo: false

wren_surv |> 
  slice_head(n = 5) |> 
  gt() |> 
  tab_header("Survival") |> 
  data_color(
    columns = ID,
    rows = ID == "543774-MMY",
    palette = c("hotpink")) |> 
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(columns = ID))

wren_repro |> 
  slice_head(n = 5) |> 
  gt() |> 
  tab_header("Reproductive Success") |> 
  data_color(
    columns = ID,
    rows = ID == "543774-MMY",
    palette = c("hotpink")) |> 
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(columns = ID))

```


## Column names

```{r}
#| echo: true

names(wren_surv)
names(wren_repro)
```

- `join_`s will use column names to merge data frames (tibbles). 
- Can join on non-matching names -- easier to `rename()` columns


## Set Operations 

```{r}
intersect(names(wren_surv), names(wren_repro))

union(names(wren_surv), names(wren_repro))
```

- Join will use `ID` by default


## Kinds of joins in `dplyr`

::: {.incremental}

- `left_join(df1, df2)` and `right_join(df1, df2)`
    - Merge one into the other by a subset of rows
    - `left_join(df1, df2)` equals `right_join(df2, df1)`
- `full_join()`: all rows, keep all values
- `inner_join()`: only matching rows

:::


## Thinking about what you want to keep

- Keep only rows in `df2` than match `df1`, duplicating information?
- Keep all the rows in `df2`, adding `NA`s for missing data?
- Keep only rows that match?


## Fairywren data

```{r}
#| echo: false

wren_surv |> 
  slice_head(n = 5) |> 
  gt() |> 
  tab_header("Survival") |> 
  data_color(
    columns = ID,
    rows = ID == "543774-MMY",
    palette = c("hotpink")) |> 
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(columns = ID))

wren_repro |> 
  slice_head(n = 5) |> 
  gt() |> 
  tab_header("Reproductive Success") |> 
  data_color(
    columns = ID,
    rows = ID == "543774-MMY",
    palette = c("hotpink")) |> 
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(columns = ID))

```


## Table dimensions

```{r}
dim(wren_surv)
dim(wren_repro)
```

- We should have 1,372 rows in the joined data


## Checking matches

Reproduction not in survival:

```{r}
setdiff(unique(wren_repro$ID), unique(wren_surv$ID))
```

Survival not in reproduction

```{r}
setdiff(unique(wren_surv$ID), unique(wren_repro$ID))
```


## Join survival to reproduction

```{r}
#| message: true

WR <- left_join(wren_repro, wren_surv)
```

```{r}
WR
```


## Join survival to reproduction

```{r}
#| message: true

WR <- left_join(wren_repro, wren_surv, by = join_by(ID))
WR
```


## Full join survival to reproduction

- `ID`s in survival data but not in reproduction data are added with `NA`s filled

```{r}
#| echo: true
#| message: true

WR <- full_join(wren_repro, wren_surv, by = join_by(ID))
WR |> filter(ID == "574717-YOO")
```


# Summarizing data and describing linear models {background-color="#40666e"}


## What information to include about the data?

::: {.incremental}

- Structure of the experimental design
    - Groups, sub-groups, etc
    - Number of measures per individual/unit
- Total number of observations (per group)
    - Initial group sizes and final group sizes
- Means and standard deviations or standard errors

:::


## What information to include about the model?

::: {.incremental}

- What would someone need to know in order to reproduce the analysis if they had the data?
    - Ideally they have your code and data
- Find examples in the literature or in this class
- Give the model a name ("linear model", "OLS regression", "*t*-test") and also describe what it does
    - "...OLS regression where age (years) is predicted by proportion of black coloration of the nose (range: 0.0 to 1.0)"
    - "...*t*-test comparing the means of eye-span (mm) between stalk-eyed flies raised on corn vs. those raised on cotton"
- Software, packages, versions

:::


## Model summaries: many options in R

- `stargazer`, `sjPlot`, `flextable`
- `knitr`, `kableExtra`, `gt`

. . .

[`modelsummary` works well](https://modelsummary.com/)^[Also install `estimatr`]

- Summary tables for observations: `datasummary_skim()`, `datasummary_balance()`, `datasummary_correlation()`, `datasummary_crosstab()`
- Summaries for linear (and other) models: `modelsummary()`
- Coefficient plots: `modelplot()`
- `output = `: save directly to file (docx, html, etc.)


## `datasummary_skim()`

```{r}
#| echo: false

LA <- read_csv("https://raw.githubusercontent.com/Middleton-Lab/abdData/refs/heads/main/inst/extdata/datasets/17/17e1LionAges.csv", show_col_types = FALSE) |> 
  rename(Age = age,
         `Proportion Black` = proportion.black)
```

```{r}
library(modelsummary)
datasummary_skim(LA)
```


## `datasummary_balance()`

```{r}
#| echo: false

HL <- read_csv("https://raw.githubusercontent.com/QM-Life-Sci/QMLS_1/refs/heads/main/data/HornedLizards.csv", show_col_types = FALSE) |> 
  drop_na() |> 
  rename(`Horn Length` = horn_length,
         Group = group) |> 
  mutate(Group = if_else(Group == "alive", "Alive", "Dead"))
```

```{r}
datasummary_balance(~ Group, data = HL)
```


## `modelsummary()`

```{r}
fm <- lm(Age ~ `Proportion Black`, data = LA)
modelsummary(list("OLS" = fm),
             estimate = "{estimate} ({conf.low}, {conf.high})",
             statistic = c("s.e. = {std.error}", 
                           "t = {statistic}",
                           "p = {p.value}"),
             gof_omit = ".*")
```



## `modelsummary()`

```{r}
fm <- lm(`Horn Length` ~ Group, data = HL)
modelsummary(list("Horn Length ~ Group" = fm),
             estimate = "{estimate} ({conf.low}, {conf.high})",
             statistic = c("t = {statistic}",
                           "p = {p.value}"),
             gof_omit = ".*",
             fmt = 2)
```


## `modelplot()`

```{r}
modelplot(fm)
```


# RMarkdown / Quarto Markdown {background-color="#40666e"}

Where we are headed...


# Questions {background-color="#f8c471"}

