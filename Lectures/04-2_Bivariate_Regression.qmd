---
title: "Unit 4: Bivariate Regression"
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
format:
  clean-revealjs:
    self-contained: true
    chalkboard: false
    standalone: true
    slide-number: true
    show-slide-number: print
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
---

```{r}
#| label: setup
#| echo: false

library(tidyverse)
library(patchwork)
library(ggExtra)
library(latex2exp)

theme_set(theme_classic(base_size = 20))

ssPlot <- function(X, Y, b, do.plot = TRUE, do.labels = TRUE) {
  n <- length(X)
  pred <- (X * b + (mean(Y) - b * mean(X)))
  SSy <- sum((Y - pred) ^ 2)
  
  M <- tibble(X, Y, pred)
  
  if (do.plot) {
    p <- ggplot() +
      geom_point(data = tibble(X = mean(X), Y = mean(Y)),
                 aes(X, Y), color = "navy", size = 7, pch = 1) +
      geom_abline(slope = b, intercept = mean(Y) - b * mean(X),
                  color = "navy", linewidth = 1) +
      geom_segment(data = M, aes(x = X, xend = X, y = Y, yend = pred),
                   color = "firebrick", linewidth = 1) +
      geom_point(data = M, aes(x = X, y = Y), size = 3)
    
    v1 <- round(b, 2)
    v2 <- round(SSy, 2)
    
    ll1 <- glue::glue("$\\theta_1 = {v1}$")
    ll2 <- glue::glue("$\\SS = {v2}$")
    
    xpos <- min(X) + 0.15 * diff(range(X))
    ypos <- min(Y) + 0.8 * diff(range(Y))
    
    if(do.labels) {
      p <- p +
        annotate(geom = "text",
                 label = TeX(ll1, output = "character"),
                 x = xpos, y = ypos,
                 parse = TRUE,
                 hjust = 0,
                 size = 9) +
        annotate(geom = "text",
                 label = TeX(ll2, output = "character"),
                 x = xpos, y = ypos * 0.96,
                 parse = TRUE,
                 hjust = 0,
                 size = 9)
    }
    print(p)
  }
  return(SSy)
}

```


# Housekeeping {background-color="#40666e"}

- Review problem set keys
- Progress Check 1 next week
    - Tuesday: Questions and review (feel free to submit ahead)
    - Thursday: In class working time


# Non-causal correlations {background-color="#40666e"}

Enjoy: [Spurious Correlations](https://www.tylervigen.com/spurious-correlations){target="_blank"}


## Non-causal correlations

![](images/vinyl.png){fig-align="center"}


## Non-causal correlations

![](images/UFO.png){fig-align="center"}


## Spurious correlations

::: {.callout-note}
## Correlation $\neq$ Causation

What else could be *separately* causing the patterns I observe?
:::

- Not restricted to correlation
- Think deeply about the processes you are studying
- Controlled experiments can help


# Bivariate regression {background-color="#40666e"}

## Coloration of lion noses

:::: {.columns}

::: {.column width="30%"}
![](images/lion_nose.png)

```{r}
LA <- read_csv("https://raw.githubusercontent.com/Middleton-Lab/abdData/refs/heads/main/inst/extdata/datasets/17/17e1LionAges.csv", show_col_types = FALSE) |> 
  rename(Age = age,
         `Proportion Black` = proportion.black)

r <- round(cor(LA$Age, LA$`Proportion Black`), digits = 2)
```

Correlation: $r$ = `r r`

:::

::: {.column width="70%"}
```{r}
#| fig-height: 8

ggplot(LA, aes(Age, `Proportion Black`)) +
  geom_point(size = 6, color = "navy") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(breaks = 1:14) +
  theme_classic(base_size = 30)
```
:::

::::


## Bivariate regression

*Predict* observed values of $y$ for observed values of $x$

```{r}
p1 <- ggplot(LA, aes(x = Age, y = `Proportion Black`)) +
  geom_point(size = 4, color = "navy") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(breaks = 1:14) +
  theme_classic(base_size = 20)

p2 <- ggplot(LA, aes(y = Age, x = `Proportion Black`)) +
  geom_point(size = 4, color = "navy") +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(breaks = 1:14) +
  theme_classic(base_size = 20)

p1 + p2
```


## For example...

1. Calibration curves (Absorption vs. Concentration)
2. Metabolic rate vs. Body mass
3. Leaf area vs. Total rainfall
4. Lion age vs. nose proportion black

Continuous variable predicted ("modeled") by one continuous variable


## Also known as

- Least squares regression
- Ordinary least squares regression
- OLS regression
- Linear regression
- "Regression"


## Simulate data

Generate $n = 30$ random data points: 

- $X \sim Normal(\mu = 10, \sigma = 1)$
- $Y = 2.3 X + \epsilon$ ("functional relationship")
- where $\epsilon \sim Normal(1, 1)$ ("noise")

```{r}
#| echo: true

set.seed(4)
n <- 30

X <- rnorm(n, mean = 10, sd = 1)
Y <- 2.3 * X + rnorm(n, mean = 1, sd = 1)

M <- tibble(X, Y)
```


## Plot data

```{r}
p <- ggplot(M, aes(X, Y)) + 
  geom_point(size = 4, color = "navy")
p
```


## Goal

What values of $\theta_0$ (intercept) and $\theta_1$ (slope) provide the best fit line through $Y$ as a function of $X$?

$$Y = \theta_0 + \theta_1 X$$

- How do we estimate $\theta_0$ and $\theta_1$ (or $\beta_0$ and $\beta_1$)?
- What defines "best fit"?


## Model properties

An infinite set of possible slopes ($\theta_1$) and intercepts ($\theta_0$)

1. Slopes and intercepts are tied together
    - Change the slope and the intercept must change
1. All lines must pass through $\left(\bar{X}, \bar{Y}\right)$.
1. Sum of the squared deviations will vary continuously.
1. **Only one** value of $\theta_1$ will minimize the sum of squares.
    - The *Ordinary Least Squares* estimate


## Assumptions of OLS 

:::: {.columns}

::: {.column width="50%"}

Few assumptions are made about $X$

- *Is* measured without error
- *Not* that it is normal or that it is randomly sampled
- Think about calibration curves. The $X$ observations are fixed.

:::

::: {.column width="50%"}

```{r, fig.height = 5, fig.width = 5}

set.seed(86)
cc <- tibble("Concentration" = seq(from = 10, to = 110, by = 10))
cc$Absorbance = cc$Concentration * 0.025 + rnorm(nrow(cc), 0, 0.15)

ggplot(cc, aes(Concentration, Absorbance)) +
  geom_point(size = 3) +
  scale_x_continuous(n.breaks = 10) +
  scale_y_continuous(limits = c(0, 3), breaks = seq(0, 3, by = 0.5))
```

:::

::::


## Minimizing Sums of Squares

```{r}
p
```


## Minimizing Sums of Squares

```{r}
y_bar <- paste("bar(Y)==", round(mean(Y), 2))
x_bar <- paste("bar(X)==", round(mean(X), 2))

p +
  geom_point(data = tibble(X = mean(X), Y = mean(Y)),
             aes(X, Y), color = "navy", size = 7, pch = 1) +
  annotate("text", x = 9.25, y= 28 ,
           label = y_bar,
           parse = TRUE,
           size = 7) +
  annotate("text", x = 9.25, y= 27 ,
           label = x_bar,
           parse = TRUE,
           size = 7)
```


## Deviates (residuals)

```{r}
# b = 0
ss <- ssPlot(X, Y, 0, do.labels = FALSE)
```


## $\theta_1 = 0$

```{r}
# b = 0
ss <- ssPlot(X, Y, 0)
```


## $\theta_1 = 0.5$

```{r}
ss <- ssPlot(X, Y, 0.5)
```


## $\theta_1 = 1$

```{r}
ss <- ssPlot(X, Y, 1)
```


## $\theta_1 = 1.5$

```{r}
ss <- ssPlot(X, Y, 1.5)
```


## $\theta_1 = 2$

```{r}
ss <- ssPlot(X, Y, 2)
```


## $\theta_1 = 2.1$

```{r}
ss <- ssPlot(X, Y, 2.1)
```


## $\theta_1 = 2.6$

```{r}
ss <- ssPlot(X, Y, 2.6)
```


## Minimizing Sums of Squares

- Search a range of values for $\theta_1$

```{r}
#| echo: true

# To hold output
SumSq <- tibble(theta_1 = seq(-10, 10, by = 0.01),
                SS = numeric(length(theta_1)))
head(SumSq)
```


## Minimizing Sums of Squares

```{r}
#| echo: true

# Iterate through slopes
for (ii in 1:nrow(SumSq)) {
  theta_1 <- SumSq$theta_1[ii]
  SumSq$SS[ii] <- ssPlot(X, Y, theta_1, do.plot = FALSE)
}

# Location of minimum SS
minSS.theta_1 <- SumSq$theta_1[which.min(SumSq$SS)]
minSS.SS <- SumSq$SS[which.min(SumSq$SS)]
```


## Minimizing Sums of Squares

```{r}
xpos <- min(SumSq$theta_1) + 0.55 * diff(range(SumSq$theta_1))
ypos <- min(SumSq$SS) + 0.3 * diff(range(SumSq$SS))

v1 <- round(minSS.theta_1, 2)
v2 <- round(minSS.SS, 2)
ll1 <- glue::glue("$\\theta_1 = {v1}$")
ll2 <- glue::glue("$\\SS = {v2}$")

ggplot() +
  geom_line(data = SumSq, aes(x = theta_1, y = SS), linewidth = 1.25) +
  geom_point(data = tibble(x = minSS.theta_1, y = minSS.SS),
             aes(x, y), color = "coral", size = 5) +
  labs(x = TeX("$\\theta_1$"),
       y = "Sum of Squares") +
  annotate(geom = "text",
           label = TeX(ll1, output = "character"),
           x = xpos, y = ypos,
           parse = TRUE,
           hjust = 0,
           size = 9) +
  annotate(geom = "text",
           label = TeX(ll2, output = "character"),
           x = xpos, y = ypos * 0.70,
           parse = TRUE,
           hjust = 0,
           size = 9)
```


## Direct calculation of $\theta_1$ and $\theta_0$

$$\theta_1 = \frac{\sum\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sum\left(X_{i}-\bar{X}\right)^{2}}$$

Numerator:  Sum of the products of *X* and *Y*

Denominator: Sum of squares of *X*

$$\theta_0 = \bar{Y} - \theta_1 \bar{X}$$


## Bivariate regression as a linear model

- Don't have to manually search for values of intercept ($\theta_0$) and slope ($\theta_1$)
    - Although we can
- Want to solve the equation directly
    - Matrix algebra

R has a function to do this: `lm()`


## Linear models: `lm()`

```{r}
#| echo: true

my_model <- lm(Y ~ X + 1, data = M)

my_model

```

- `+ 1` includes an intercept (default, optional)
- For each 1 unit increase in $X$, $Y$ increases $2.39525$
- $Y = -0.03541$ when $X = 0$


## Model formulas: `Y ~ X`

:::{.center}
"Y *is modeled by* X"
:::

- Response (outcome) variable: $Y$
- Predictor variable: $X$
- Equivalent to: $Y = \theta_0 + \theta_1 X$
- Intercept (`1`) is assumed
    - `Y ~ X + 1`


## $\theta_1 = 2.39525$

```{r}
ss <- ssPlot(X, Y, 2.39525)
```


# Questions {background-color="#f8c471"}

